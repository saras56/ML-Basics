{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4a6f56",
   "metadata": {},
   "source": [
    "## Import and read data, data split and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f474b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset or read data\n",
    "\n",
    "1) df=pd.read_csv('zomato.csv',encoding='latin-1')\n",
    "2) df=sns.load_dataset('tips') # from seaborn\n",
    "3) df=pd.read_excel('Country-Code.xlsx')\n",
    "4) dataset=pd.DataFrame(boston.data,columns=boston.feature_names)#converting sklearn downloaded dataset to pandas dataframe.\n",
    "5) dataset['Price']=boston.target #creating a new colomn\n",
    "6) from sklearn.datasets import load_boston\n",
    "   boston=load_boston()\n",
    "7) data = pd.read_csv('creditcard.csv',sep=',')\n",
    "8) df=pd.read_csv('air_quality.csv',parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Dependent and independent split\n",
    "1) X= dataset.iloc[:,:-1]\n",
    "   y=dataset.iloc[:,-1]\n",
    "2) X=df.iloc[:,1:]    \n",
    "   y=df['total_bill']\n",
    "3) X=df.drop(\"Class\",axis=1)\n",
    "   y=df.Class\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "1) X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "2) X_train,X_test,y_train,y_test=train_test_split(data[['Age','Fare']].fillna(0),data['Survived'],test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b5947",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df.shape\n",
    "Df.columns\n",
    "Df.info()\n",
    "Df.describe()\n",
    "df['Age'].unique()\n",
    "df.B=df.B.astype(int) # converting a column datatype to int\n",
    "df.drop(['User_ID'],axis=1,inplace=True)  \n",
    "#axis=1 will delete the data colomnwise, axis = 0 means row wise.\n",
    "#inplace=True : Once the userId is deleted, it will update automatically to the df\n",
    "final_df=pd.merge(df,df_country,on='Country Code',how='left')  # merge 2 files\n",
    "df=df_train.append(df_test) # append a file to the bottom of another (Refer Black Friday study)\n",
    "df['Stay_In_Years']=df['Stay_In_Years'].str.replace('+','')  # Replace or neglect unnecessary stuff\n",
    "\n",
    "# define numerical & categorical columns\n",
    "numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "\n",
    "# print columns\n",
    "print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))\n",
    "\n",
    "# take out numerical columns alone for processing (checking for 0 values, replace with NaN etc)\n",
    "df1=df.select_dtypes(include=['float64', \"int64\"])\n",
    "#OR\n",
    "new_df = df.select_dtypes(include=np.number)\n",
    "\n",
    "# check no of zeroes (or any other null values) in each columns. We will convert this to NaN for imputation\n",
    "for col in df.columns:\n",
    "    print (col + \":\" + str((df[col] == 0).sum()))\n",
    "    \n",
    "df.isin(['-']).any()  # Search for specific character in dataframe\n",
    "\n",
    "# Replace zero or any invalid character to NaN\n",
    "df.replace(0, np.nan, inplace=True)  # whole dataframe\n",
    "# selected columns\n",
    "cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin']\n",
    "df[cols] = df[cols].replace({'0':np.nan, 0:np.nan})\n",
    "\n",
    "\n",
    "# Handling categorical features - One hot encoding/ target guided ordinal encoding\n",
    "df['Gender'] = df['Gender'].map({'F':0,'M':1}) \n",
    "# best approach to convert a categorical variable to one hot encoded using map function\n",
    "\n",
    "df['Age'] = df['Age'].map({'0-17':1, '55+':7, '26-35':3, '46-50':5, '51-55':6, '36-45':4, '18-25':2}) # target guided ordinal encoding (Refer: Black Friday)\n",
    "\n",
    "df_city=pd.get_dummies(df['City_Category'],drop_first=True)\n",
    "df=pd.concat([df,df_city],axis=1)\n",
    "df=df.drop('City_Category',axis=1)\n",
    "\n",
    "# Label encoding\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['Age']= label_encoder.fit_transform(df['Age'])\n",
    " df['Age'].unique()\n",
    "\n",
    "# Correlation\n",
    "dataset.corr() # very important incase of regression problems\n",
    "# Reg plot, Pair plot and scatter plots are used to visually analyse the relatioship between independent and target feature\n",
    "\n",
    "\n",
    "# Pandas operations\n",
    "\n",
    "# Q: Find the countries name that has given zero rating\n",
    "final_df[final_df[\"Rating color\"]=='White'].groupby('Country').size().reset_index().rename(columns={0:'Rating Count'})\n",
    "\n",
    "# Q: Find out which currency used by which country\n",
    "final_df[[\"Country\",\"Currency\"]].groupby([\"Country\",\"Currency\"]).size().reset_index().rename(columns={0:'Count'})\n",
    "\n",
    "# Q: Which all countries do have online deliveries?\n",
    "final_df[final_df[\"Has Online delivery\"]=='Yes'].Country.value_counts()\n",
    " \n",
    "# Q: Find top 10 cuisines\n",
    "data=final_df[[\"Cuisines\"]].groupby(\"Cuisines\").size().reset_index().rename(columns={0:'Count'})\n",
    "data.sort_values(by='Count', ascending=False).head(10)\n",
    "\n",
    "# Q: We have a feature of date in format 06/09/2019. Create a derived feature which stores Date, month and year separately ie separate features.\n",
    "df['Date']=df['Date_of_Journey'].str.split('/').str[0] \n",
    "df['Month']=df['Date_of_Journey'].str.split('/').str[1]\n",
    "final_df['Year']=df['Date_of_Journey'].str.split('/').str[2]\n",
    "# efficient way of doing the same operation using lambda function\n",
    "df[\"Date\"]=df['Date_of_Journey'].apply(lambda x:x.split(\"/\")[0])\n",
    "df[\"Month\"]=df['Date_of_Journey'].apply(lambda x:x.split(\"/\")[1])\n",
    "df[\"Year\"]=df['Date_of_Journey'].apply(lambda x:x.split(\"/\")[2])\n",
    "\n",
    "# Q: Separate out the hour and min from the following time format. 01:10 22 Mar (Here we need 01 as one feature and 10 as another)\n",
    "final_df['Time'] = final_df['Time'].apply(lambda x : x.split(' ')[0]) # There will be total of 3 indeces data. 0th index after split is time.\n",
    "final_df['Hour']= final_df['Time'].apply(lambda x : x.split(':')[0])\n",
    "final_df['Min']= final_df['Time'].apply(lambda x : x.split(':')[1])\n",
    "\n",
    "# Q: A feature of time in format 2h 15m. Convert this into minutes. \n",
    "#Note:Few entries contains only hour portion and delete the entries which has only minutes. (Refer flight Prediction EDA)\n",
    "df['duration_hour']=final_df['Duration'].str.split(' ').str[0].str.split('h').str[0]  #took out hour portion alone\n",
    "df['duration_hour']=final_df['duration_hour'].astype('int')  # This will give error cos as mentioned in note, few entries contains only minutes.\n",
    "df[final_df['duration_hour']=='5m']\n",
    "df.drop(6474,axis=0,inplace=True) # droping that row which contains only min\n",
    "df['duration_hour']=final_df['duration_hour'].astype('int')\n",
    "final_df['Duration_min']=final_df['Duration'].str.split(' ').str[1].str.split('m').str[0]  # taking out min portion of time\n",
    "final_df['Duration_min']=final_df['Duration_min'].fillna(0)  # few etries has only hours\n",
    "final_df['Duration_min']=final_df['Duration_min'].astype('int')\n",
    "final_df['Duration_new'] = final_df['Duration_min']+(final_df['duration_hour']*60)  # converting into minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLING OUTLIERS\n",
    "\n",
    "figure=df.Fare.hist(bins=50)\n",
    "df.boxplot(column=\"Fare\")\n",
    "df['Fare'].describe()\n",
    "\n",
    "# If the feature follows a Gaussian Distribution, we will calculate the boundaries which differentiates the outliers\n",
    "\n",
    "uppper_boundary=df['Age'].mean() + 3* df['Age'].std()\n",
    "lower_boundary=df['Age'].mean() - 3* df['Age'].std()\n",
    "print(lower_boundary), print(uppper_boundary),print(df['Age'].mean())\n",
    "\n",
    "# If features are skewed\n",
    "# Lets compute the Interquantile range to calculate the boundaries\n",
    "IQR=df.Fare.quantile(0.75)-df.Fare.quantile(0.25)\n",
    "lower_bridge=df['Fare'].quantile(0.25)-(IQR*1.5)\n",
    "upper_bridge=df['Fare'].quantile(0.75)+(IQR*1.5)\n",
    "#### Extreme outliers - This would be practically applicable based on feature\n",
    "lower_bridge=df['Fare'].quantile(0.25)-(IQR*3)\n",
    "upper_bridge=df['Fare'].quantile(0.75)+(IQR*3)\n",
    "\n",
    "# Outliers removing procedure\n",
    "data.loc[data['Age']>=73,'Age']=73  # Assume 73 is the outlier boundary. Data above 73 is replaced with 73\n",
    "data.loc[data['Fare']>=100,'Fare']=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a73c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36045824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLING IMBALANCED DATASET \n",
    "\n",
    "# Refer the que in Applied AI interview que doc\n",
    "\n",
    "df['target'].value_counts() # check for imbalancing\n",
    "# Best algo in case of imbalanced dataset would be choosing ensemble algos like GBDT/RF. \n",
    "# CV and Hyper parameter tuned RF having a paramter of class_weight is an efficient approach (if we have large dataset)\n",
    "class_weight=dict({0:1,1:100})  # 0 and 1 are class labels.giving a class weight since its imbalanced.0 label is given weight of 1\n",
    "# class label 1 is given a weight of 100. class weight should be given in form of dictionary\n",
    "classifier=RandomForestClassifier(class_weight=class_weight) # all other parameters are set as default. Change as per requirement\n",
    "\n",
    "# UNDERSAMPLING\n",
    "from collections import Counter\n",
    "Counter(y_train)\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "ns=NearMiss(0.8) # Suppose label 0 is dominant than 1. After nearmiss(0.8), no of datapoints of 1 would be 80% of 0, ie 0 label's data has been reduced\n",
    "X_train_ns,y_train_ns=ns.fit_sample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_ns)))\n",
    "\n",
    "\n",
    "# OVERSAMPLING\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os=RandomOverSampler(0.75)\n",
    "X_train_ns,y_train_ns=os.fit_sample(X_train,y_train)\n",
    "\n",
    "# SMOTETomek\n",
    "from imblearn.combine import SMOTETomek\n",
    "os=SMOTETomek(0.75)\n",
    "X_train_ns,y_train_ns=os.fit_sample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb3ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLING MISSING DATA\n",
    "\n",
    "df.isnull().sum()\n",
    "df[df['Embarked'].isnull()] \n",
    "data.isnull().values.any()\n",
    "[features for features in df.columns if df[features].isnull().sum()>0]   # checking for missing data using list comprehension \n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #displaying missing data \n",
    "##find the percentage of null values\n",
    "df['cabin_null']=np.where(df['Cabin'].isnull(),1,0) # replacing null values with 1 and other zero.\n",
    "df['cabin_null'].mean()\n",
    "\n",
    "# Age,Cabin and embarkked in titanic dataset has missing values. Considering domain knowledge and understandings, we can see that\n",
    "# both Age and Cabin have some relationship (MNAR).But embarked is missing completely at Random \n",
    "\n",
    "# MCAR \n",
    "# Imputation (Mean,median,mode,random sample) - MCAR - Refer Handling missing values Krish\n",
    "# Random sample imputation is a very good approach cos it doesnt distort the variance like mean/median imputation\n",
    "df[\"Product_Cat\"]=df[\"Product_Cat\"].fillna(df[\"Product_Cat\"].mode()[0])  # replacing missing values with mode\n",
    "# Another approach for multiple features\n",
    "\n",
    "def impute_nan(df,feature,median):\n",
    "    df[feature+\"_median\"]=df[feature].fillna(median)\n",
    "    df[feature+\"_random\"]=df[feature]\n",
    "    ##It will have the random sample to fill the na\n",
    "    random_sample=df[feature].dropna().sample(df[feature].isnull().sum(),random_state=0)\n",
    "    ##pandas need to have same index in order to merge the dataset\n",
    "    random_sample.index=df[df[feature].isnull()].index\n",
    "    df.loc[df[feature].isnull(),feature+'_random']=random_sample\n",
    "    \n",
    "\n",
    "# Note: Random sample imputation doesnt work well in every problem. So test and try\n",
    "\n",
    "\n",
    "# Missing Not At Random (MNAR) : Capturing NaN with new feature and end of distribution imputation\n",
    "\n",
    "# Capturing NaN with new feature (only disadv is curse of dimensionality. This will work well if dimensions are less)\n",
    "df['Age_NAN']=np.where(df['Age'].isnull(),1,0)\n",
    "df['Age'].fillna(df.Age.median(),inplace=True)\n",
    "\n",
    "# end of distribution imputation\n",
    "\n",
    "extreme=df.Age.mean()+3*df.Age.std()\n",
    "def impute_nan(df,variable,median,extreme):\n",
    "    df[variable+\"_end_distribution\"]=df[variable].fillna(extreme)\n",
    "    df[variable].fillna(median,inplace=True)\n",
    "# we are doing median imputation alongwith is inorder to compare both techniques\n",
    "df[variable+\"_end_distribution\"]=df[variable].fillna(extreme) # Check the histogram and box plot after imputation\n",
    "\n",
    "# KNN Imputer\n",
    "\n",
    "cols = ['Age','Survived'] ### Defining Cols\n",
    "from sklearn.impute import KNNImputer\n",
    "knn = KNNImputer(n_neighbors=5)\n",
    "knn.fit_transform(df[cols])\n",
    "df2 = pd.DataFrame(knn.transform(df[cols]),columns=['Age', 'Survived'])\n",
    "titanic['Age_KNN'] = df2['Age']\n",
    "titanic[titanic.Age.isna()][['Age','Age_KNN']] # Prints the Nan with index in original Height column and MICE imputed replacements\n",
    "\n",
    "#Linear Regression Imputer\n",
    "\n",
    "train_data = df[df['Age'].notna()]\n",
    "test_data = df[df[\"Age\"].isnull()]\n",
    "X_train= train_data.drop('Age',axis=1)\n",
    "y_train = train_data['Age']\n",
    "X_test = test_data.drop('Age',axis=1)\n",
    "y_test = test_data['Age']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "a = y_train.to_list()\n",
    "b = list(y_pred)\n",
    "a.extend(b)\n",
    "titanic['Age_LR'] = a\n",
    "\n",
    "# MICE\n",
    "\n",
    "pip install impyute\n",
    "from impyute.imputation.cs import mice\n",
    "data_imputed=mice(df.values) # start the MICE training\n",
    "cleaned_df = pd.DataFrame(data_imputed,columns=['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Age']) \n",
    "# Should give all the columns\n",
    "titanic['Age_MICE'] = cleaned_df['Age']\n",
    "\n",
    "\n",
    "# Plot the distribution differences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "df['Age'].plot(kind='kde', ax=ax)\n",
    "df.Age_median.plot(kind='kde', ax=ax, color='red')\n",
    "df.Age_random.plot(kind='kde', ax=ax, color='green')\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e600a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca171b18",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1212ac7",
   "metadata": {},
   "source": [
    "####   https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/#h2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ad059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CORRELATION \n",
    "\n",
    "# separate dataset into train and test inorder to avoid overfitting\n",
    "X_train.corr()\n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = X_train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
    "plt.show()\n",
    "\n",
    "# with the following function we can select highly correlated features\n",
    "# it will remove the first feature that is correlated with anything other feature\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "corr_features = correlation(X_train, 0.7)\n",
    "len(set(corr_features))\n",
    "X_train.drop(corr_features,axis=1)\n",
    "X_test.drop(corr_features,axis=1)\n",
    "\n",
    "\n",
    "# INFORMATION GAIN - MUTUAL INFORMATION In Classification\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "mutual_info\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)\n",
    "#let's plot the ordered mutual_info values per feature\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "#we Will select the  top 5 important features\n",
    "sel_five_cols = SelectKBest(mutual_info_classif, k=5)\n",
    "sel_five_cols.fit(X_train, y_train)\n",
    "X_train.columns[sel_five_cols.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e3b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c0b9d11",
   "metadata": {},
   "source": [
    "## Mean, Median Mode,plots, PDE, Histogram, Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df.total_bill) # mean\n",
    "np.median(df['total_bill']) # median\n",
    "statistics.mode(df['total_bill'])  # mode\n",
    "\n",
    "sns.boxplot(df['total_bill']) #box plot for seeing outliers\n",
    "sns.histplot(df['total_bill'])  # histogram\n",
    "sns.histplot(df['total_bill'], kde=True) # to plot PDE\n",
    "sns.countplot(df['smoker'])  # count plot\n",
    "sns.regplot(x=\"RM\",y=\"Price\",data=dataset) # Regression plot\n",
    "\n",
    "# 2D scatter plot with color coding\n",
    "sns.set_style(\"whitegrid\");\n",
    "sns.FacetGrid(iris, hue=\"species\", size=4) \\\n",
    "   .map(plt.scatter, \"sepal_length\", \"sepal_width\") \\\n",
    "   .add_legend();\n",
    "plt.show();\n",
    "\n",
    "# pair plot : pairwise scatter plot. Only possible to view 2D patterns\n",
    "plt.close();\n",
    "sns.set_style(\"whitegrid\");\n",
    "sns.pairplot(iris, hue=\"species\", size=3);\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis') #displaying missing data \n",
    "\n",
    "# Create a pie chart for top 5 cities distribution\n",
    "city_names = final_df.City.value_counts().index\n",
    "city_count = final_df[\"City\"].value_counts().values\n",
    "plt.pie(city_count[:5],labels=city_names[:5],autopct='%1.2f%%')\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,6)\n",
    "sns.barplot(x='Aggregate rating',y='Rating Count',hue='Rating color',palette=['white','red','orange','yellow','green','green'],data=ratings)\n",
    "sns.countplot(x=\"Rating text\",data=ratings,palette=['white','red','orange','yellow','green','green'])\n",
    "\n",
    "\n",
    "np.percentile(df['tip'],[25,75]) \n",
    "np.percentile(medians,97.5) \n",
    "# Methods for finding outliers - z test, IQR (Refer Krish Stats note)\n",
    "\n",
    "# PDF \n",
    "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
    "   .map(sns.distplot, \"sepal_width\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4101e",
   "metadata": {},
   "source": [
    "### Distributions, QQ plot, PDF and CDF, Box cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc = 20, scale = 5, size=100) #loc is mean, scale is std dev, size is sample size\n",
    "y= np.random.exponential(scale=1.0, size=1000) # exp dist\n",
    "z= np.random.pareto(a=2.0, size=1000)\n",
    "\n",
    "# QQ PLot\n",
    "def plot_data(df,feature):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1,2,2)\n",
    "    stat.probplot(df[feature],dist='norm',plot=pylab)\n",
    "    plt.show()\n",
    "# OR\n",
    "stats.probplot(x, dist=\"norm\", plot=plt) # or sp.stats.probplot(x, dist=\"norm\", plot=matplotlib.pyplot)\n",
    "plt.grid()\n",
    "# There will be a slight error in result. Its because of scipy wont fetch its module by itself, need to import it.\n",
    "# If we standardize the trasformed values (box cox or any other tramsformation, the red straight line in QQ plot would be 45 degree. \n",
    "# Otherwise the angle differs.The intention here is just to know whether both distributions are aligned properly. \n",
    "\n",
    "# plot PDF(y)\n",
    "sns.set()\n",
    "ax = sns.distplot(y)\n",
    "plt.show()\n",
    "\n",
    "# plot CDF(X)\n",
    "kwargs = {'cumulative': True}\n",
    "sns.distplot(x, hist_kws=kwargs, kde_kws=kwargs)\n",
    "\n",
    "# Box cox transform\n",
    "x_t, l = stats.boxcox(x) # l=lambda variable of box-cox trasform, x_t =x tranformed by box-cox\n",
    "print(l)  #If l is zero it is a special case of box-cox which is log-normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2081f1",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "###### Steps: \n",
    "- Define data.Which can be from any distribution. Given data, we need to estimate population statistic (eg median)\n",
    "- Write function to generate a bootstrap sample (sampling with repalcement) of size n given a sample S\n",
    "- calculate the median of each bootstrap sample\n",
    "- Take median of medians for the population estimate\n",
    "- Calculate percentiles for estimating confidence interval\n",
    "\n",
    "#### For bootstrap code refer \"Q-Q_plot, Box-cox, Bootstrapping, PR & KS test\" note in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ad151",
   "metadata": {},
   "source": [
    "# Permutation test and KS test ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f7fac",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/06/feature-selection-using-statistical-tests/\n",
    "\n",
    "Notes on Central limit theorem, Hypothesis testing etc are need to be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8f5a87",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar=StandardScaler()\n",
    "\n",
    "X_train_scaled=scalar.fit_transform(X_train)\n",
    "X_test_scaled=scalar.transform(X_test)\n",
    "\n",
    "classification.fit(X_train_scaled,y_train)\n",
    "classification.predict(X_test_scaled)\n",
    "\n",
    "# Min Max scaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "df_minmax=pd.DataFrame(min_max.fit_transform(X_train))\n",
    "\n",
    "# Robust Scaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler=RobustScaler()\n",
    "df_robust_scaler=pd.DataFrame(scaler.fit_transform(X_train))\n",
    "\n",
    "# GAUSSIAN TRANSFORMATION\n",
    "\n",
    "# Some machine learning algorithms like linear and logistic assume that the features are normally distributed -Accuracy -Performance\n",
    "#### If you want to check whether feature is guassian or normal distributed\n",
    "#### Q-Q plot\n",
    "def plot_data(df,feature):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1,2,2)\n",
    "    stat.probplot(df[feature],dist='norm',plot=pylab)\n",
    "    plt.show()\n",
    "\n",
    "## Box Cox Trannsformation\n",
    "df['Age_Boxcox'],parameters=stat.boxcox(df['Age'])\n",
    "print(parameters)\n",
    "\n",
    "## Log transformation\n",
    "df['Age_log']=np.log(df['Age'])\n",
    "plot_data(df,'Age_log')\n",
    "\n",
    "## Exponential Transdormation\n",
    "df['Age_exponential']=df.Age**(1/1.2)\n",
    "\n",
    "## Square Root Transformation\n",
    "df['Age_sqaure']=df.Age**(1/2)\n",
    "\n",
    "# Reciprocal trans\n",
    "df['Age_reciprocal']=1/df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff9ae5",
   "metadata": {},
   "source": [
    "## Model training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123487b",
   "metadata": {},
   "source": [
    "**Q: Which machine learning model to choose?**\n",
    "\n",
    "A: Experiment with all the ML algorithms and check the performances using metrices. Then choose the best model and do hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae02a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "metrics = np.zeros((8,5))  # For plotting purposes\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "models = {\n",
    "    \"Nearest Neighbors\" : KNeighborsClassifier(), \n",
    "    \"Linear SVM\" : SVC(kernel=\"linear\"), \n",
    "    \"RBF SVM\" : SVC(),\n",
    "    \"Decision Tree\" : DecisionTreeClassifier(), \n",
    "    \"Random Forest\" : RandomForestClassifier(), \n",
    "    \"Neural Net\" : MLPClassifier(), \n",
    "    \"AdaBoost\" : AdaBoostClassifier(),\n",
    "    \"Naive Bayes\" : GaussianNB()\n",
    "}\n",
    "\n",
    "#print (list(models)\n",
    "\n",
    "X, y = make_moons(noise=0.3, random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model.fit(X_train,y_train)\n",
    "    # Make preeictions\n",
    "    y_train_pred= model.predict(X_train)\n",
    "    y_test_pred= model.predict(X_test)\n",
    "    \n",
    "    # training performances\n",
    "    train_accuracy= accuracy_score(y_train,y_train_pred)\n",
    "    train_f1 = f1_score(y_train,y_train_pred,average='weighted')\n",
    "    train_precision= precision_score(y_train,y_train_pred)\n",
    "    train_recall= recall_score(y_train,y_train_pred)\n",
    "    train_rocauc_score= roc_auc_score(y_train,y_train_pred)\n",
    "    \n",
    "    #test data performance\n",
    "    test_accuracy= accuracy_score(y_test,y_test_pred)\n",
    "    test_f1 = f1_score(y_test,y_test_pred,average='weighted')\n",
    "    test_precision= precision_score(y_test,y_test_pred)\n",
    "    test_recall= recall_score(y_test,y_test_pred)\n",
    "    test_rocauc_score= roc_auc_score(y_test,y_test_pred)\n",
    "    \n",
    "    metrics[i,:]= [test_accuracy,test_f1,test_precision,test_recall,test_rocauc_score]\n",
    "    \n",
    "    print (list(models.keys())[i])\n",
    "    \n",
    "    print (\"Model Performance of training set\")\n",
    "    print (\"- Accuracy : {:.4f}\".format(train_accuracy))\n",
    "    print (\"- F1 Score : {:.4f}\".format(train_f1))\n",
    "    print (\"- Precision : {:.4f}\".format(train_precision))\n",
    "    print (\"- Recall : {:.4f}\".format(train_recall))\n",
    "    print (\"- ROC AUC Score : {:.4f}\".format(train_rocauc_score))\n",
    "    \n",
    "    print ('-'*35)\n",
    "    \n",
    "    print (\"Model Performance of test set\")\n",
    "    print (\"- Accuracy : {:.4f}\".format(test_accuracy))\n",
    "    print (\"- F1 Score : {:.4f}\".format(test_f1))\n",
    "    print (\"- Precision : {:.4f}\".format(test_precision))\n",
    "    print (\"- Recall : {:.4f}\".format(test_recall))\n",
    "    print (\"- ROC AUC Score : {:.4f}\".format(test_rocauc_score))\n",
    "           \n",
    "    print('='*35)\n",
    "    print('\\n')\n",
    "    \n",
    "a=np.arange(8)\n",
    "w=0.15\n",
    "fig,ax=plt.subplots(figsize=(20,8),edgecolor='k')\n",
    "p1=ax.bar(a,metrics[:,0],w,color='b')\n",
    "p2=ax.bar(a-w,metrics[:,1],w,color='g')\n",
    "p3=ax.bar(a+w,metrics[:,2],w,color='y')\n",
    "p4=ax.bar(a-2*w,metrics[:,3],w,color='r')\n",
    "p5=ax.bar(a+2*w,metrics[:,4],w,color='c')\n",
    "ax.set_xticks(a)\n",
    "ax.set_xticklabels(('KNN','Linear SVM','RBF SVM','DT','RF','NN','AdaB','GNN'))\n",
    "ax.set_title('Test data performance')\n",
    "ax.legend((p1[0],p2[0],p3[0],p4[0],p5[0]),('Accuracy','F1 score','Precision','Recall','ROC AUC Score'))\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Performance')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "                    \n",
    "# Note : From the performance metrics analysis, we can shortlist the better performing models further for hyperparameter tuning\n",
    "\n",
    "# Hyper parameter training\n",
    "\n",
    "rf_params = {\"max_depth\": [5,8,15,None,10],\n",
    "             \"max_features\": [5,7,\"auto\",8],\n",
    "             \"min_samples_split\": [2,8,15,20],\n",
    "             \"n_estimators\": [100,200,500,1000]}\n",
    "# Models list for hyperparameter tuning\n",
    "randomcv_models = [\n",
    "    (\"RF\", RandomForestClassifier(), rf_params),\n",
    "]\n",
    "randomcv_models\n",
    "\n",
    "# Training stage : using random search CV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model_param ={}\n",
    "\n",
    "for name,model,params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                               param_distributions=params,\n",
    "                               n_iter=100,\n",
    "                               cv=3,\n",
    "                               verbose=2,\n",
    "                               n_jobs=-1)\n",
    "    random.fit(X_train,y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "    \n",
    "    for model_name in model_param:\n",
    "        print(f\"------------------Best parameters for {model_name}-----------------\")\n",
    "        print(model_param[model_name])\n",
    "    \n",
    "\n",
    "# Train the model using the best parameters\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\":RandomForestClassifier(n_estimators= 1000, min_samples_split= 2, max_features= 'auto', max_depth= 8)\n",
    "}\n",
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model.fit(X_train,y_train) # train model\n",
    "    \n",
    "    # make predictions\n",
    "    y_train_pred= model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate training and test performance metrices and then print the o/p as shown above\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256452a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM WITH CROSS VALIDATION AND HYPER PARAMETER TUNING USING GRIDSEARCH AND RANDOMSEARCH\n",
    "\n",
    "# RandomSearch CV\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
    "# GridSearch CV\n",
    "# Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "\n",
    "from time import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "# Display the last digit\n",
    "plt.figure(1, figsize=(5, 5))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.show()\n",
    "\n",
    "# To apply one classifier to this data, we need to flatten the image, to turn the data into a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "print (n_samples)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# GridserachCV internally splits D_train into 5-fold cross validation. cv=None set default 5-fold cross val. we can customize this.\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "#score function is the performance metric. Because business doesnt understand loss function. \n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )  # defining the model\n",
    "    clf.fit(X_train, y_train)  # fitting the model\n",
    "\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print(\"Grid scores on train set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))   # refer cv_results for details. This part simply prining the data from results.\n",
    "\n",
    "    print(\"Detailed classification report on test-set:\")\n",
    "    #print(\"The model is trained on the full development set.\")\n",
    "    #print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))  \n",
    "    #support in classification report means how many datapoints did we use for getting the precision and all.\n",
    "#Q:Why do we calculate mean and std dev as the main purpose of GridsearchCV is o find best parameters which we get through best_params_ ??\n",
    "#A:As default 5-fold cv. ie for a single set of hyperparameters, 5 models are trained.so 5 precision and recall values and 1 mean and 1 std dev.\n",
    "\n",
    "\n",
    "# Random search CV\n",
    "X, y = load_digits(return_X_y=True)\n",
    "# build a classifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='elasticnet', fit_intercept=True)\n",
    "# Defining a classifier which uses SGD. Here its SVM with regulizer as lambda(L1+rL2). Elasticnet can be any combinnation of L1 and L2 as per the classifier chosen.\n",
    "# fit_intercept=True means we need to train not only for W but also b.\n",
    "# SGD+hinge performs very similar as libSVM optimizer (SVC with linear kernal). libSVM is a very efficient optimizaion library.there are a lo of optimizers. \n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i)) \n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "# One hack : Since cv_results is dictionary, instead of writing all the above code we can make a panda dataframe out of it for displaying.\n",
    "# In the above Grid search code, we explicitly mentioned the scores (precision and recall) but here we didnt mention any. So whats the score here?\n",
    "# RandomizedsearchCV() function has score=None means it takes default scoring of classifier. Here classifier is SGDClassifier where default scoring is mean Accuray. \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'average': [True, False],\n",
    "              'l1_ratio': stats.uniform(0, 1),\n",
    "              'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'average': [True, False],\n",
    "              'l1_ratio': np.linspace(0, 1, num=10),  \n",
    "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}   \n",
    "# 10^-4 to 10^1\n",
    "# L1 ratio = 1 --> use only L1 , 0 use L2\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)\n",
    "\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION WITH K CROSS VALIDATION\n",
    "\n",
    "log_class=LogisticRegression()\n",
    "grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']}\n",
    "cv=KFold(n_splits=5,random_state=None,shuffle=False)\n",
    "clf=GridSearchCV(log_class,grid,cv=cv,n_jobs=-1,scoring='f1_macro') # Study the difference between f1_macro and f1_weighted\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# LINEAR REGRESSION\n",
    "\n",
    "regression=LinearRegression()\n",
    "regression.fit(X_train,y_train)\n",
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "regression.get_params() # On which parameters the model has been trained\n",
    "reg_pred= regression.predict(X_test)  # Prediction on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e65e1",
   "metadata": {},
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSION\n",
    "plt.scatter(y_test,reg_pred)  # If plot is somewhat linear and aligned, model is performing well\n",
    "# Residuals or error\n",
    "residuals=y_test-reg_pred\n",
    "# plot the residuals\n",
    "sns.displot(residuals,kind='kde')\n",
    "#residuals are kind of normally distributed, mainly ranges from -10 to +10. Few errors are more than 10 which can be outliers or genuine model prediction error.\n",
    "\n",
    "\n",
    "# CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44967ae",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX, ACCURACY, CALSSIFICATION REPORT\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# MSE, MAE, RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_squared_error(y_test,reg_pred))\n",
    "print(mean_absolute_error(y_test,reg_pred))\n",
    "print(np.sqrt(mean_squared_error(y_test,reg_pred))) # RMSE\n",
    "\n",
    "## R SQUARE, ADJUSTED R SQUARE\n",
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,reg_pred)  #  Value near to 1, more better the score is\n",
    "#adjusted R-squared\n",
    "1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1) ## adjusted R^2 value should be less than R^2 \n",
    "\n",
    "\n",
    "# PLot ROC AUC Curve - Automated\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "plt.figure()\n",
    "\n",
    "# Add the models to the list you want to view on the roc plot\n",
    "\n",
    "auc_models = [\n",
    "    {\n",
    "        'label':'Random Forest Classifer',\n",
    "        'model':RandomForestClassifier(n_estimators= 1000, min_samples_split= 2, max_features= 'auto', max_depth= 8),\n",
    "        'auc':0.9499\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create loop through all models\n",
    "\n",
    "for algo in auc_models:\n",
    "    model=algo['model']  # select the model\n",
    "    model.fit(X_train,y_train) # train the model\n",
    "# Compute FPR and TPR\n",
    "    fpr,tpr,thresholds = roc_curve(y_test,model.predict_proba(X_test)[:,1])\n",
    "# Compute area under the curve to display in the plot\n",
    "    plt.plot(fpr,tpr,label='%s ROC (area=%0.2f)' % (algo['label'],algo['auc']))\n",
    "# Custom settings for the plot\n",
    "    plt.plot([0,1], [0,1],'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('Specificity (False Positive Rate)')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(\"auc.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b923a18",
   "metadata": {},
   "source": [
    "## Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"classifier.pkl\",\"wb\") # pickle file should be open in wb mode - write byte\n",
    "pickle.dump(classifier, pickle_out)\n",
    "pickle_out.close() \n",
    "# Since we have not given any path,if this file doesnt exist in my local folder, wb will create the pikle file in our currect running directory\n",
    "\n",
    "# testing the pickle file whether its predicting, by giving a single input ie data[0]\n",
    "pickled_model=pickle.load(open('regmodel.pkl','rb'))\n",
    "pickled_model.predict(scalar.transform(boston.data[0].reshape(1,-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
